{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "AUC : 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import utils\n",
    "import pywt\n",
    "\n",
    "\n",
    "def wav_to_log_mel(wav_path, sr, n_fft, win_length, hop_length, n_mels, power):\n",
    "    # mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "    #     sample_rate=sr,\n",
    "    #     n_fft=n_fft,\n",
    "    #     win_length=win_length,\n",
    "    #     hop_length=hop_length,\n",
    "    #     n_mels=n_mels,\n",
    "    #     power=power,\n",
    "    # )\n",
    "\n",
    "    mel_transform = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        power=power,\n",
    "        normalized = True\n",
    "    )\n",
    "\n",
    "    # wav_data, _ = torchaudio.load(wav_path)\n",
    "    File = pd.read_csv(wav_path)\n",
    "    amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "    \n",
    "    wav_data = torch.Tensor(File.values[:,0])\n",
    "    mel_spec = mel_transform(wav_data)\n",
    "    # mel_spec = torch.transpose(mel_transform(wav_data), 0, 1)\n",
    "\n",
    "    DbScale = amp_to_db(mel_spec)\n",
    "    X = torch.Tensor(mel_spec[:, :512])\n",
    "    MinMax_X = torch.Tensor(MinMaxScaler().fit_transform(X))\n",
    "    \n",
    "    wav_data = torch.Tensor(File.values[:,1])\n",
    "    mel_spec = mel_transform(wav_data)\n",
    "    # mel_spec = torch.transpose(mel_transform(wav_data), 0, 1)\n",
    "    DbScale = amp_to_db(mel_spec)\n",
    "    Y = torch.Tensor(mel_spec[:, :512])\n",
    "    MinMax_Y = torch.Tensor(MinMaxScaler().fit_transform(Y))\n",
    "    \n",
    "\n",
    "    mel_spec = torch.concat((X, MinMax_X), dim = 1)\n",
    "\n",
    "    # noise_wav = utils.add_noise_to_wav(wav_data, -4)\n",
    "\n",
    "    # mel_spec = mel_transform(noise_wav)\n",
    "    \n",
    "    # mel_spec = mel_transform(wav_data)\n",
    "    return mel_spec\n",
    "\n",
    "\n",
    "def get_train_loader(args):\n",
    "    train_dir = args.train_dir\n",
    "    sr = args.sr\n",
    "    n_fft = args.n_fft\n",
    "    win_length = args.win_length\n",
    "    hop_length = args.hop_length\n",
    "    n_mels = args.n_mels\n",
    "    power = args.power\n",
    "\n",
    "    file_list = os.listdir(train_dir)\n",
    "    file_list.sort()\n",
    "    file_list = [os.path.join(train_dir, file) for file in file_list if 'csv' in file]\n",
    "    train_dataloader = BaselineDataLoader(\n",
    "        file_list, sr, n_fft, win_length, hop_length, n_mels, power\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataloader,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.n_workers,\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def get_eval_loader(args):\n",
    "    eval_dir = args.eval_dir\n",
    "    sr = args.sr\n",
    "    n_fft = args.n_fft\n",
    "    win_length = args.win_length\n",
    "    hop_length = args.hop_length\n",
    "    n_mels = args.n_mels\n",
    "    power = args.power\n",
    "    \n",
    "    file_list = os.listdir(eval_dir)\n",
    "    file_list.sort()\n",
    "    file_list = [os.path.join(eval_dir, file) for file in file_list if 'csv' in file]\n",
    "    eval_dataloader = BaselineDataLoader(\n",
    "        file_list, sr, n_fft, win_length, hop_length, n_mels, power\n",
    "    )\n",
    "\n",
    "    eval_loader = DataLoader(\n",
    "        eval_dataloader, batch_size=1, shuffle=False, num_workers=0\n",
    "    )\n",
    "    return eval_loader, file_list\n",
    "\n",
    "\n",
    "def get_test_loader(args):\n",
    "    test_dir = args.test_dir\n",
    "    sr = args.sr\n",
    "    n_fft = args.n_fft\n",
    "    win_length = args.win_length\n",
    "    hop_length = args.hop_length\n",
    "    n_mels = args.n_mels\n",
    "    power = args.power\n",
    "\n",
    "    file_list = os.listdir(test_dir)\n",
    "    file_list.sort()\n",
    "    file_list = [os.path.join(test_dir, file) for file in file_list if 'csv' in file]\n",
    "    eval_dataloader = BaselineDataLoader(\n",
    "        file_list, sr, n_fft, win_length, hop_length, n_mels, power\n",
    "    )\n",
    "\n",
    "    eval_loader = DataLoader(\n",
    "        eval_dataloader, batch_size=1, shuffle=False, num_workers=0\n",
    "    )\n",
    "    return eval_loader, file_list\n",
    "\n",
    "\n",
    "class BaselineDataLoader(Dataset):\n",
    "    def __init__(self, data_path, sr, n_fft, win_length, hop_length, n_mels, power):\n",
    "        self.data_path = data_path\n",
    "        self.sr = sr\n",
    "        self.n_fft = n_fft\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.power = power\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.data_path[idx]\n",
    "        log_mel_spec = wav_to_log_mel(\n",
    "            wav_path,\n",
    "            self.sr,\n",
    "            self.n_fft,\n",
    "            self.win_length,\n",
    "            self.hop_length,\n",
    "            self.n_mels,\n",
    "            self.power,\n",
    "        )\n",
    "\n",
    "        anomaly_label = utils.get_fault_label(wav_path)\n",
    "\n",
    "        return log_mel_spec, anomaly_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# net.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, in_size, latent_size):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(int(in_size), int(in_size/2))\n",
    "    self.linear2 = nn.Linear(int(in_size/2), int(in_size/4))\n",
    "    self.linear3 = nn.Linear(int(in_size/4), latent_size)\n",
    "    self.relu = nn.ReLU(True)\n",
    "        \n",
    "  def forward(self, w):\n",
    "    out = self.linear1(w)\n",
    "    z1 = self.relu(out)\n",
    "    out = self.linear2(out)\n",
    "    z2 = self.relu(out)\n",
    "    out = self.linear3(out)\n",
    "    z3 = self.relu(out)\n",
    "    return z1, z2, z3\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, latent_size, out_size):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(latent_size, int(out_size/4))\n",
    "    self.linear2 = nn.Linear(int(out_size/4), int(out_size/2))\n",
    "    self.linear3 = nn.Linear(int(out_size/2), out_size)\n",
    "    self.relu = nn.ReLU(True)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "  def forward(self, z1, z2, z3):\n",
    "    out = self.linear1(z3)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear2(out + z2)\n",
    "    out = self.relu(out)\n",
    "    out= self.linear3(out + z1)\n",
    "    w = self.sigmoid(out)\n",
    "    return w\n",
    "\n",
    "\n",
    "class AutoencoderModel(nn.Module):\n",
    "    def __init__(self, w_size, z_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(w_size, z_size)\n",
    "        self.decoder1 = Decoder(z_size, w_size)\n",
    "        self.decoder2 = Decoder(z_size, w_size)\n",
    "  \n",
    "    def forward(self, batch, n): \n",
    "        z1, z2, z3 = self.encoder(batch)\n",
    "        w1 = self.decoder1(z1, z2, z3)\n",
    "        w2 = self.decoder2(z1, z2, z3)\n",
    "        Z1, Z2, Z3 = self.encoder(w1)\n",
    "        w3 = self.decoder2(Z1, Z2, Z3)\n",
    "        loss1 = 1/n*torch.mean((batch-w1)**2)+(1-1/n)*torch.mean((batch-w3)**2)\n",
    "        loss2 = 1/n*torch.mean((batch-w2)**2)-(1-1/n)*torch.mean((batch-w3)**2)\n",
    "        return loss1,loss2\n",
    "\n",
    "\n",
    "def Autoencoder(W = 202, Z = 32):\n",
    "    return AutoencoderModel(W, Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# eval.py\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import train\n",
    "import net\n",
    "import dataset\n",
    "import utils\n",
    "\n",
    "def testing(model, test_data, alpha=.5, beta=.5):\n",
    "    with torch.no_grad():\n",
    "        z1, z2, z3 = model.encoder(test_data)\n",
    "        w1=model.decoder1(z1, z2, z3)\n",
    "        Z1, Z2, Z3 = model.encoder(w1)\n",
    "        w2=model.decoder2(Z1, Z2, Z3)\n",
    "        A = alpha*torch.mean((test_data-w1)**2, axis = (1,2)).cpu()+beta*torch.mean((test_data-w2)**2,axis = (1,2)).cpu()\n",
    "        B = torch.mean((test_data-w1)**2, axis = (1,2)).cpu()\n",
    "        C = beta*torch.mean((test_data-w2)**2,axis = (1,2)).cpu()\n",
    "    return A, B, C\n",
    "\n",
    "def eval(args):\n",
    "    print(\"Evaluation started...\")\n",
    "    os.makedirs(args.result_dir, exist_ok=True)\n",
    "\n",
    "    model = net.Autoencoder().cuda()\n",
    "    model_path = os.path.join(args.model_dir, args.model_path)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    dataloader, file_list = dataset.get_eval_loader(args)\n",
    "\n",
    "    score_list = [[\"File\", \"Score\"]]\n",
    "\n",
    "    fault_label = []\n",
    "    y_true, y_predA, y_predB, y_predC = [], [], [], []\n",
    "    \n",
    "\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        log_mel = data[0].cuda()\n",
    "        true_label = data[1].cuda()\n",
    "\n",
    "        A, B, C = testing(model, log_mel)\n",
    "         \n",
    "        fault_label.append(true_label.item())\n",
    "\n",
    "        y_true.append(1 if data[1].item() > 0 else 0)\n",
    "        y_predA.append(A)\n",
    "        y_predB.append(B)\n",
    "        y_predC.append(C)\n",
    "\n",
    "        file_name = os.path.splitext(file_list[idx].split(\"/\")[-1])[0]\n",
    "\n",
    "        # score_list.append([file_name, loss])\n",
    "    y_predA = np.array(y_predA, dtype = np.float32)\n",
    "    y_predB = np.array(y_predB, dtype = np.float32)\n",
    "    y_predC = np.array(y_predC, dtype = np.float32)\n",
    "    auc = metrics.roc_auc_score(np.array(y_true), y_predA)\n",
    "    print(\"AUC: \", auc)\n",
    "    auc = metrics.roc_auc_score(np.array(y_true), y_predB)\n",
    "    print(\"AUC: \", auc)\n",
    "    auc = metrics.roc_auc_score(np.array(y_true), y_predC)\n",
    "    print(\"AUC: \", auc)\n",
    "\n",
    "    # y_predB = (y_predB - np.min(y_predB)) / (np.max(y_predB) - np.min(y_predB))\n",
    "    # auc = metrics.roc_auc_score(y_true, y_predB)\n",
    "    # print(\"AUC: \", auc)\n",
    "    # utils.save_csv(score_list, os.path.join(args.result_dir, \"eval_score.csv\"))\n",
    "\n",
    "    # fault_types = [\"normal\", \"inner\", \"outer\", \"ball\"]\n",
    "\n",
    "    # for fault in fault_types:\n",
    "    #     if fault == \"normal\":\n",
    "    #         continue\n",
    "    #     else:\n",
    "    #         fault_indices = [\n",
    "    #             i\n",
    "    #             for i, label in enumerate(fault_label)\n",
    "    #             if (label == fault_types.index(fault) or label == 0)\n",
    "    #         ]\n",
    "    #         pred_labels = [y_pred[i] for i in fault_indices]\n",
    "    #         true_labels = [y_true[i] for i in fault_indices]\n",
    "    #         fault_auc = metrics.roc_auc_score(true_labels, pred_labels)\n",
    "    #         print(f\"{fault} AUC: {fault_auc}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir('E:/DataChallenge/usad-master/Baseline')\n",
    "    args = train.get_args()\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n",
    "\n",
    "    eval(args)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
